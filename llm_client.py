"""LLM access layer using LangChain chat models with optional fallback."""

from __future__ import annotations

import json
import re
import time
from typing import Any

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_openai import ChatOpenAI

from pipeline_utils import PipelineConfig, PipelineError, fill_template
from prompts import JSON_REPAIR_SYS, JSON_REPAIR_USR


def clean_json_text(text: str) -> str:
    t = text.strip()
    t = re.sub(r"^```(?:json)?", "", t, flags=re.IGNORECASE).strip()
    t = re.sub(r"```$", "", t).strip()
    return t


def extract_json_candidate(text: str) -> str | None:
    s = clean_json_text(text)
    try:
        json.loads(s)
        return s
    except Exception:
        pass

    start = None
    depth = 0
    for i, ch in enumerate(s):
        if ch in "[{" and start is None:
            start = i
            depth = 1
            continue
        if start is None:
            continue
        if ch in "[{":
            depth += 1
        elif ch in "]}":
            depth -= 1
            if depth == 0:
                candidate = s[start : i + 1]
                try:
                    json.loads(candidate)
                    return candidate
                except Exception:
                    start = None
                    depth = 0
    return None


def validate_keys(obj: dict[str, Any], required_keys: list[str], label: str) -> None:
    missing = [k for k in required_keys if k not in obj]
    if missing:
        raise PipelineError(f"{label} missing keys: {missing}")


def parse_json_or_raise(text: str, label: str) -> dict[str, Any]:
    candidate = extract_json_candidate(text)
    if not candidate:
        raise PipelineError(f"{label}: no valid JSON object found")
    data = json.loads(candidate)
    if not isinstance(data, dict):
        raise PipelineError(f"{label}: JSON root must be object")
    return data


def is_token_limit_error(exc: Exception) -> bool:
    msg = str(exc).lower()
    token_markers = [
        "token",
        "context length",
        "maximum context length",
        "context window",
        "max_tokens",
        "too many tokens",
        "prompt is too long",
        "exceeds the context",
    ]
    return any(m in msg for m in token_markers)


def shrink_prompt_text(text: str) -> str:
    if len(text) <= 12000:
        return text
    keep_head = int(len(text) * 0.55)
    keep_tail = int(len(text) * 0.35)
    if keep_head + keep_tail >= len(text):
        return text
    marker = "\n\n[...TRUNCATED_FOR_TOKEN_LIMIT...]\n\n"
    return text[:keep_head] + marker + text[-keep_tail:]


def _message_to_text(message: Any) -> str:
    content = getattr(message, "content", "")
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts: list[str] = []
        for item in content:
            if isinstance(item, str):
                parts.append(item)
            elif isinstance(item, dict):
                txt = item.get("text") or item.get("content")
                if isinstance(txt, str):
                    parts.append(txt)
        return "\n".join(parts).strip()
    return str(content or "")


class LLMClient:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        self.typhoon_llm: ChatOpenAI | None = None
        self.ollama_chat_llm: ChatOllama | None = None
        self.embedder: OllamaEmbeddings | None = None
        self.call_log: list[dict[str, Any]] = []
        self._ollama_fallback_warned = False
        self._ollama_only_mode = False

        if cfg.typhoon_api_key and cfg.typhoon_base_url:
            self.typhoon_llm = ChatOpenAI(
                model=cfg.typhoon_model,
                api_key=cfg.typhoon_api_key,
                base_url=cfg.typhoon_base_url,
                temperature=0.1,
                max_tokens=max(int(cfg.typhoon_max_tokens or 8192), 256),
                timeout=cfg.llm_timeout_sec,
            )

        self.ollama_chat_llm = ChatOllama(
            model=cfg.ollama_chat_model,
            base_url=cfg.ollama_base_url,
            temperature=0.1,
            num_predict=8192,
        )
        self.embedder = OllamaEmbeddings(
            model=cfg.ollama_embed_model,
            base_url=cfg.ollama_base_url,
        )
        self._ollama_only_mode = self.typhoon_llm is None

        if not self.typhoon_llm and not cfg.allow_ollama_chat_fallback:
            raise PipelineError(
                "Typhoon is unavailable and chat fallback is disabled "
                "(set ALLOW_OLLAMA_CHAT_FALLBACK=true for Option B)."
            )

    def _providers_in_order(self) -> list[str]:
        providers: list[str] = []
        if self.typhoon_llm:
            providers.append("typhoon")
        if self.cfg.allow_ollama_chat_fallback:
            providers.append("ollama")
        return providers

    def _invoke_typhoon(self, system: str, user: str, json_mode: bool) -> str:
        if not self.typhoon_llm:
            raise PipelineError("Typhoon provider is not configured")

        dynamic_max_tokens = max(int(self.cfg.typhoon_max_tokens or 8192), 256)
        dynamic_user = user
        last_exc: Exception | None = None

        for _ in range(5):
            try:
                model = self.typhoon_llm.bind(max_tokens=dynamic_max_tokens)
                if json_mode:
                    model = model.bind(response_format={"type": "json_object"})
                resp = model.invoke(
                    [
                        SystemMessage(content=system),
                        HumanMessage(content=dynamic_user),
                    ]
                )
                content = _message_to_text(resp)
                if not content:
                    raise PipelineError("Typhoon returned empty response")
                return content
            except Exception as exc:
                last_exc = exc
                if not is_token_limit_error(exc):
                    raise
                if dynamic_max_tokens > 1024:
                    dynamic_max_tokens = max(1024, dynamic_max_tokens // 2)
                    continue
                shrunk = shrink_prompt_text(dynamic_user)
                if shrunk != dynamic_user:
                    dynamic_user = shrunk
                    continue
                break

        raise PipelineError(f"Typhoon token-limit handling exhausted: {last_exc}")

    def _invoke_ollama(self, system: str, user: str, json_mode: bool = False) -> str:
        if not self.ollama_chat_llm:
            raise PipelineError("Ollama chat model is not configured")
        model = self.ollama_chat_llm
        if json_mode:
            # Ask Ollama runtime to bias structured JSON output.
            model = model.bind(format="json")

        resp = model.invoke(
            [
                SystemMessage(content=system),
                HumanMessage(content=user),
            ]
        )
        content = _message_to_text(resp)
        if not content:
            raise PipelineError("Ollama returned empty response")
        return content

    def _repair_json(self, broken: str, required_keys: list[str], tag: str) -> dict[str, Any] | None:
        try:
            return parse_json_or_raise(broken, f"{tag}/repair-heuristic")
        except Exception:
            pass

        repair_prompt = fill_template(
            JSON_REPAIR_USR,
            REQUIRED_KEYS=json.dumps(required_keys, ensure_ascii=False),
            BROKEN_JSON=broken[:60000],
        )

        for provider in self._providers_in_order():
            start = time.time()
            try:
                if provider == "typhoon":
                    raw = self._invoke_typhoon(JSON_REPAIR_SYS, repair_prompt, json_mode=False)
                else:
                    raw = self._invoke_ollama(JSON_REPAIR_SYS, repair_prompt, json_mode=True)
                parsed = parse_json_or_raise(raw, f"{tag}/repair-{provider}")
                validate_keys(parsed, required_keys, f"{tag}/repair-{provider}")
                self.call_log.append(
                    {
                        "tag": tag,
                        "provider": provider,
                        "phase": "repair",
                        "chat_fallback_used": provider == "ollama",
                        "success": True,
                        "latency_sec": round(time.time() - start, 3),
                    }
                )
                return parsed
            except Exception as exc:
                self.call_log.append(
                    {
                        "tag": tag,
                        "provider": provider,
                        "phase": "repair",
                        "chat_fallback_used": provider == "ollama",
                        "success": False,
                        "latency_sec": round(time.time() - start, 3),
                        "error": str(exc),
                    }
                )
        return None

    def call(
        self,
        system: str,
        user: str,
        *,
        json_mode: bool,
        required_keys: list[str] | None,
        tag: str,
    ) -> dict[str, Any] | str:
        last_error: Exception | None = None
        required = required_keys or []

        for attempt in range(1, self.cfg.llm_max_retries + 1):
            for provider in self._providers_in_order():
                start = time.time()
                raw = ""
                try:
                    if provider == "typhoon":
                        raw = self._invoke_typhoon(system, user, json_mode=json_mode)
                    else:
                        if not self._ollama_fallback_warned:
                            if self._ollama_only_mode:
                                print("ℹ️ Ollama-only mode active (Typhoon not configured).")
                            else:
                                print(
                                    "⚠️ Using Ollama chat fallback (quality may differ from Typhoon). "
                                    "Set ALLOW_OLLAMA_CHAT_FALLBACK=false for Typhoon-only mode."
                                )
                            self._ollama_fallback_warned = True
                        raw = self._invoke_ollama(system, user, json_mode=json_mode)

                    if not json_mode:
                        self.call_log.append(
                            {
                                "tag": tag,
                                "provider": provider,
                                "attempt": attempt,
                                "chat_fallback_used": provider == "ollama",
                                "success": True,
                                "latency_sec": round(time.time() - start, 3),
                            }
                        )
                        return raw

                    parsed = parse_json_or_raise(raw, tag)
                    if required:
                        validate_keys(parsed, required, tag)

                    self.call_log.append(
                        {
                            "tag": tag,
                            "provider": provider,
                            "attempt": attempt,
                            "chat_fallback_used": provider == "ollama",
                            "success": True,
                            "latency_sec": round(time.time() - start, 3),
                        }
                    )
                    return parsed
                except Exception as exc:
                    last_error = exc
                    self.call_log.append(
                        {
                            "tag": tag,
                            "provider": provider,
                            "attempt": attempt,
                            "chat_fallback_used": provider == "ollama",
                            "success": False,
                            "latency_sec": round(time.time() - start, 3),
                            "error": str(exc),
                            "raw_preview": (raw[:300] if raw else ""),
                        }
                    )
                    if json_mode and raw:
                        repaired = self._repair_json(raw, required, tag)
                        if repaired is not None:
                            return repaired

        raise PipelineError(f"{tag} failed after retries: {last_error}")

    def embed(self, texts: list[str]) -> list[list[float]]:
        if not self.embedder:
            raise PipelineError("Embedding model is not configured")
        try:
            vectors = self.embedder.embed_documents(texts)
            return [list(v) for v in vectors]
        except Exception as exc:
            raise PipelineError(f"Embedding failed: {exc}") from exc
